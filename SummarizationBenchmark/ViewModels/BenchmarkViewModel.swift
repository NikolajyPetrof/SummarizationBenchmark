//
//  BenchmarkViewModel.swift
//  SummarizationBenchmark
//
//  Created by Nikolay Petrov on 10.06.2025.
//

import Foundation
import MLX
import MLXLLM
import MLXLMCommon

@MainActor
class BenchmarkViewModel: ObservableObject {
    @Published var selectedModel: SummarizationModel?
    @Published var inputText = ""
    @Published var generatedSummary = ""
    @Published var isGenerating = false
    @Published var currentResult: BenchmarkResult?
    @Published var sessions: [BenchmarkSession] = []
    @Published var currentSession: BenchmarkSession?
    @Published var errorMessage: String?
    
    private let modelManager: ModelManager
    private let sessionManager = BenchmarkSessionManager()
    
    init(modelManager: ModelManager) {
        self.modelManager = modelManager
        loadSessions()
    }
    
    // Factory method to create BenchmarkViewModel instance
    static func create(with modelManager: ModelManager) -> BenchmarkViewModel {
        return BenchmarkViewModel(modelManager: modelManager)
    }
    
    func startNewSession(name: String, type: BenchmarkSession.SessionType = .custom) {
        currentSession = BenchmarkSession(
            name: name,
            type: type,
            results: []
        )
        
        // Save session to the list
        if let session = currentSession {
            sessions.append(session)
            saveSessionsToFile()
        }
    }
    
    @MainActor
    func runBenchmark(text: String, model: SummarizationModel, batchSize: Int = 1) async throws {
        guard let container = modelManager.loadedModels[model.modelId] else {
            throw SummarizerError.modelNotLoaded
        }
        
        isGenerating = true
        let startTime = CFAbsoluteTimeGetCurrent()
        
        // Reset peak memory before measurement
        MLX.GPU.resetPeakMemory()
        
        do {
            // –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –±–∞—Ç—á–∞ —Ç–µ–∫—Å—Ç–æ–≤
            let batchTexts: [String] = batchSize > 1 
                ? Array(repeating: text, count: batchSize)
                : [text]
            
            print("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ —Ä–∞–∑–º–µ—Ä–æ–º \(batchSize) —Ç–µ–∫—Å—Ç–æ–≤")
            
            // –°–±—Ä–∞—Å—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –∏–∑–º–µ—Ä–µ–Ω–∏–µ–º –±–∞—Ç—á–∞
            MLX.GPU.resetPeakMemory()
            let memoryBeforeBatch = getCurrentMemoryUsage()
            print("üìä –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ –±–∞—Ç—á–µ–º: \(memoryBeforeBatch) MB")
            
            var summaries: [(String, Int)] = []
            
            // –î–ª—è —Ä–µ–∞–ª—å–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è –Ω–∞ –ø–∞–º—è—Ç—å –±–∞—Ç—á–∞ –Ω—É–∂–Ω–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –∑–∞–≥—Ä—É–∑–∏—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤ –º–æ–¥–µ–ª–∏ –≤ –ø–∞–º—è—Ç—å
            if batchSize > 1 {
                // –ó–∞–≥—Ä—É–∑–∫–∞ –ø–∞–º—è—Ç–∏ –¥–ª—è —Å–∏–º—É–ª—è—Ü–∏–∏ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –±–∞—Ç—á–∞
                // –°–æ–∑–¥–∞–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ç–µ–Ω–∑–æ—Ä–æ–≤ –≤ –ø–∞–º—è—Ç–∏ GPU –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±–∞—Ç—á–∞
                print("üì¶ –ò–º–∏—Ç–∞—Ü–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ –±–∞—Ç—á–∞ \(batchSize) –≤ –ø–∞–º—è—Ç—å...")
                
                // –ü–æ–¥–≥–æ—Ç–æ–≤–∏–º –≤—Å–µ –ø—Ä–æ–º–ø—Ç—ã —Å—Ä–∞–∑—É
                _ = batchTexts.map { model.configuration.createPrompt(for: $0) }
                
                // –ó–¥–µ—Å—å –º—ã –¥–æ–ª–∂–Ω—ã –≤—ã–¥–µ–ª–∏—Ç—å –ø–∞–º—è—Ç—å –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ
                // –î–ª—è —ç—Ç–æ–≥–æ —Å–æ–∑–¥–∞–¥–∏–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–Ω–∑–æ—Ä—ã –≤ –ø–∞–º—è—Ç–∏
                
                // MLX –Ω–µ –∏–º–µ–µ—Ç –ø—Ä—è–º–æ–≥–æ –±–∞—Ç—á–∞ –≤ API, –ø–æ—ç—Ç–æ–º—É –∏–º–∏—Ç–∏—Ä—É–µ–º –∑–∞–≥—Ä—É–∑–∫—É –ø–∞–º—è—Ç–∏
                // —Å–æ–∑–¥–∞–Ω–∏–µ–º –∫—Ä—É–ø–Ω–æ–≥–æ —Ç–µ–Ω–∑–æ—Ä–∞
                let dummyShapeSize = 1024 * 1024 * batchSize
                let _ = MLX.full([dummyShapeSize], values: 1.0) // –û–∫–æ–ª–æ 4MB –∑–∞ –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
            }
            
            print("üíæ –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∏–º–∏—Ç–∞—Ü–∏–∏ –±–∞—Ç—á–∞: \(getCurrentMemoryUsage()) MB")
            
            // –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ (—Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞)
            for (i, batchText) in batchTexts.enumerated() {
                print("üîÅ –û–±—Ä–∞–±–æ—Ç–∫–∞ —ç–ª–µ–º–µ–Ω—Ç–∞ \(i+1)/\(batchSize) –±–∞—Ç—á–∞")
                
                let prompt = model.configuration.createPrompt(for: batchText)
                let batchSummary = try await container.perform { context in
                    let input = try await context.processor.prepare(input: .init(prompt: prompt))
                    
                    let generateStream = try MLXLMCommon.generate(
                        input: input,
                        parameters: GenerateParameters(
                            maxTokens: 200,
                            temperature: 0.7,
                            topP: 0.9,
                            repetitionPenalty: 1.1
                        ),
                        context: context
                    )
                    
                    var localGeneratedText = ""
                    var localTokenCount = 0
                    
                    for await generation in generateStream {
                        if let chunk = generation.chunk {
                            localGeneratedText += chunk
                            localTokenCount += 1
                        }
                    }
                    
                    return (localGeneratedText, localTokenCount)
                }
                
                print("üìä –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ —ç–ª–µ–º–µ–Ω—Ç–∞ \(i+1): \(getCurrentMemoryUsage()) MB")
                summaries.append(batchSummary)
            }
            
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–≤–æ–µ —Ä–µ–∑—é–º–µ –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ UI
            let summary = summaries.first ?? ("", 0)
            
            // –°—É–º–º–∏—Ä—É–µ–º —Ç–æ–∫–µ–Ω—ã —Å–æ –≤—Å–µ—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤ –±–∞—Ç—á–∞ –¥–ª—è —Ä–∞—Å—á–µ—Ç–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
            let totalTokens = summaries.reduce(0) { $0 + $1.1 }
            let endTime = CFAbsoluteTimeGetCurrent()
            
            // Get peak memory after inference
            let memoryUsed = getCurrentMemoryUsage()
            
            // –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏
            let inferenceTime = endTime - startTime
            
            // –°—É–º–º–∞—Ä–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã –∏ —Ç–æ–∫–µ–Ω—ã –Ω–∞ –æ–¥–Ω–æ —Ä–µ–∑—é–º–µ (–ø–µ—Ä–≤–æ–µ)
            let tokensCount = summary.1
            
            // –¢–æ–∫–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥—É —É—á–∏—Ç—ã–≤–∞—é—Ç –æ–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
            let tokensPerSecond = Double(totalTokens) / inferenceTime
            
            // –°–∂–∞—Ç–∏–µ —Ç–µ–∫—Å—Ç–∞ –±–µ—Ä–µ–º –∏–∑ –ø–µ—Ä–≤–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            let compressionRatio = Double(summary.0.count) / Double(text.count)
            
            // –ü–æ–ª—É—á–∞–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –ø–∞–º—è—Ç–∏ –∏–∑ MLX.GPU API
            let peakLoadMemory = Double(MLX.GPU.peakMemory) / 1024 / 1024 // MB
            
            // –ü–æ–ª—É—á–∞–µ–º —Ç–∏–ø –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–∏
            let quantType = model.configuration.additionalMetadata["quantization"] ?? "unknown"
            
            // –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º –ø–∞–º—è—Ç—å –Ω–∞ –æ–¥–∏–Ω —ç–ª–µ–º–µ–Ω—Ç –±–∞—Ç—á–∞
            let memoryPerItem = batchSize > 1 ? memoryUsed / Double(batchSize) : memoryUsed
            
            // –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–º—è—Ç–∏ (—Ç–æ–∫–µ–Ω—ã –Ω–∞ –º–µ–≥–∞–±–∞–π—Ç)
            let memEfficiency = memoryUsed > 0 ? Double(totalTokens) / memoryUsed * 100 : 0
            
            // –°–æ–∑–¥–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –ø–∞–º—è—Ç–∏
            let metrics = BenchmarkResult.PerformanceMetrics(
                loadTime: 0, // Will be measured separately
                inferenceTime: inferenceTime,
                tokensPerSecond: tokensPerSecond,
                memoryUsed: memoryUsed,
                peakLoadMemory: peakLoadMemory,
                peakInferenceMemory: peakLoadMemory, // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ –∂–µ –∑–Ω–∞—á–µ–Ω–∏–µ, —Ç.–∫. —Ç–æ—á–Ω–æ–µ –ø–∏–∫–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –º–æ–≥–ª–æ –∏–∑–º–µ–Ω–∏—Ç—å—Å—è
                memoryPerBatchItem: memoryPerItem,
                batchSize: batchSize,
                quantizationType: quantType,
                memoryEfficiency: memEfficiency,
                summaryLength: summary.0.count,
                compressionRatio: compressionRatio,
                inputLength: text.count,
                tokensGenerated: tokensCount
            )
            
            let result = BenchmarkResult(
                timestamp: Date(),
                modelName: model.name,
                modelId: model.modelId,
                inputText: text,
                generatedSummary: summary.0,
                metrics: metrics
            )
            
            currentResult = result
            generatedSummary = summary.0
            
            // Add to current session
            if var session = currentSession {
                session.results.append(result)
                currentSession = session
            }
            
        } catch {
            throw error
        }
        
        isGenerating = false
    }
    
    private func getCurrentMemoryUsage() -> Double {
        let memoryInfo = modelManager.getMemoryInfo()
        let usedMB = Double(memoryInfo.used) / (1024 * 1024) // Convert to MB
        return usedMB
    }
    
    func saveSession() {
        guard let session = currentSession else { return }
        sessions.append(session)
        
        // Save to UserDefaults or file
        saveSessionsToFile()
    }
    
    private func saveSessionsToFile() {
        sessionManager.saveSessions(sessions)
    }
    
    func loadSessions() {
        sessions = sessionManager.loadSessions()
        
        if let lastSession = sessions.last {
            currentSession = lastSession
        }
    }
    
    func clearCurrentResult() {
        currentResult = nil
        generatedSummary = ""
        errorMessage = nil
    }
    
    func deleteSession(_ session: BenchmarkSession) {
        sessions.removeAll { $0.id == session.id }
        if currentSession?.id == session.id {
            currentSession = nil
        }
        saveSessionsToFile()
    }
    
    func exportResults(from session: BenchmarkSession) -> URL? {
        return sessionManager.exportSessionAsCSV(session)
    }
}

// MARK: - Errors

enum SummarizerError: Error, LocalizedError {
    case modelNotLoaded
    case generationFailed(String)
    case invalidInput
    case memoryError
    
    var errorDescription: String? {
        switch self {
        case .modelNotLoaded:
            return "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π."
        case .generationFailed(let message):
            return "–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: \(message)"
        case .invalidInput:
            return "–ù–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–≤–µ—Ä—å—Ç–µ –≤–≤–æ–¥."
        case .memoryError:
            return "–û—à–∏–±–∫–∞ –ø–∞–º—è—Ç–∏. –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ GPU –ø–∞–º—è—Ç–∏ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –æ–ø–µ—Ä–∞—Ü–∏–∏."
        }
    }
}
